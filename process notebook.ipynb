{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from functools import reduce\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('./data/cloze.ipynb', 'r')\n",
    "nb = f.read()\n",
    "nbd = json.loads(nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "cells = [ c for c in nbd['cells'] ]\n",
    "get_cells = lambda e: [c for c in cells if c['cell_type'] == e]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_code = get_cells('code')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['%reload_ext autoreload\\n', '%autoreload 2\\n', '%matplotlib inline']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "txt = c_code[0]['source']; txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt.append('cloze')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def isCloze(txtArr, cloze_marker=r'cloze', checkEmpty=False):\n",
    "    present = [re.search(cloze_marker,s) for s in txtArr]\n",
    "    match = None\n",
    "    if len(present):\n",
    "        match = reduce(lambda c, acc: c if c else acc, present)\n",
    "    \n",
    "    if match is not None:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "isCloze(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_cloze = ['before\\n', '# cloze {\\n', 'code', '# }\\n']\n",
    "no_cloze = ['before\\n', '# cl \\n', 'code', '# } c\\n']\n",
    "no_cloze_empty = ['before\\n', '# cl \\n', 'code', '# } c\\n', '']\n",
    "has_cloze_empty = ['before\\n', '# cl \\n', 'code', '# } c\\n', '']\n",
    "empty = []\n",
    "\n",
    "assert(isCloze(has_cloze) == True)\n",
    "assert(isCloze(no_cloze) == False)\n",
    "assert(isCloze(no_cloze_empty) == False)\n",
    "assert(isCloze(has_cloze_empty) == False )\n",
    "assert(isCloze(empty) == False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cell_type': 'code',\n",
       " 'execution_count': 43,\n",
       " 'metadata': {},\n",
       " 'outputs': [],\n",
       " 'source': ['class DeepNN(nn.Module):\\n',\n",
       "  '    ### cloze {\\n',\n",
       "  '    def __init__(self, layers):\\n',\n",
       "  '        super().__init__()\\n',\n",
       "  '        print(layers)\\n',\n",
       "  '        self.layers = {}\\n',\n",
       "  '        for i in range(len(layers) -1):\\n',\n",
       "  '            print(i, [layers[i], layers[i+1]])\\n',\n",
       "  '            self.layers[f\"lin_{i}\"] = nn.Linear(layers[i], layers[i+1])\\n',\n",
       "  '    def forward(self, xb):\\n',\n",
       "  '        ab = xb\\n',\n",
       "  '        for i in range(len(layers) -1):\\n',\n",
       "  '            ab = F.relu(self.layers[f\"lin_{i}\"](ab))\\n',\n",
       "  '        return ab\\n',\n",
       "  '    ### } cloze ']}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_code[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "isCloze(c_code[14]['source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_cloze_re = r'# cloze {'\n",
    "end_cloze_re = r'# }'\n",
    "\n",
    "# should return cloze code lines and the mutated source array\n",
    "\n",
    "def extractCloze(cell_source):\n",
    "    start_i = None\n",
    "    end_i = None\n",
    "    output = []\n",
    "    for i, x in enumerate(cell_source):\n",
    "        \n",
    "        if isCloze([x], start_cloze_re):\n",
    "            start_i = i \n",
    "        \n",
    "        if start_i is not None and isCloze([x], end_cloze_re):\n",
    "            end_i = i\n",
    "            \n",
    "            if not end_i - start_i == 1:\n",
    "#                 print(\"EXTRACTING...\")\n",
    "                cloze_block = cell_source[start_i+1:end_i] # content of cloze tag\n",
    "                sliced_source = cell_source[:start_i+1] + cell_source[end_i:] # includes cloze tag\n",
    "\n",
    "                output.append((cloze_block, sliced_source))\n",
    "\n",
    "            # reset detection\n",
    "            start_i = None\n",
    "            end_i = None\n",
    "             \n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['code1\\n', 'code2\\n', 'code3\\n', 'code4\\n', 'code5\\n', 'code6\\n', 'code7\\n', 'code8\\n', 'code9\\n', 'code10\\n'], ['before\\n', '# cloze {\\n', '# }\\n'])\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "code_lines = [ f'code{i+1}\\n' for i in range(10) ]\n",
    "has_cloze = ['before\\n', '# cloze {\\n'] + code_lines  + ['# }\\n']\n",
    "deleted_cloze = ['before\\n', '# cloze {\\n'] + ['# }\\n']\n",
    "\n",
    "print(extractCloze(has_cloze)[0])\n",
    "assert(extractCloze(has_cloze)[0][0] == code_lines)\n",
    "\n",
    "print(extractCloze(deleted_cloze))\n",
    "assert(extractCloze(deleted_cloze) == []) # assume you return nothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "['before\\n', '# cloze {\\n', '# }\\n', '# cloze {\\n', 'code1_2\\n', 'code2_2\\n', '# }\\n', 'random end\\n']\n",
      "1\n",
      "['before\\n', '# cloze {\\n', 'code1_1\\n', 'code2_1\\n', '# }\\n', '# cloze {\\n', '# }\\n', 'random end\\n']\n"
     ]
    }
   ],
   "source": [
    "# if there are 2 clozes blocks within 1 cell\n",
    "# num of cloze in one cell\n",
    "num = 2\n",
    "\n",
    "code_lines = lambda y: [ f'code{i+1}_{y}\\n' for i in range(2) ]\n",
    "cloze_list = [ ['# cloze {\\n', *code_lines(i+1), '# }\\n'] for i in range(num) ]\n",
    "cloze_list_flat = [l for sublist in cloze_list for l in sublist]\n",
    "has_multi_cloze = ['before\\n', *cloze_list_flat, 'random end\\n']\n",
    "\n",
    "\n",
    "# print(extractCloze(has_multi_cloze)[1])\n",
    "processed = extractCloze(has_multi_cloze)\n",
    "for i in range(num):\n",
    "    print(i)\n",
    "    clz, cln_src = processed[i]\n",
    "    assert(clz == code_lines(i+1))\n",
    "    print(cln_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['    def __init__(self, layers):\\n',\n",
       "   '        super().__init__()\\n',\n",
       "   '        print(layers)\\n',\n",
       "   '        self.layers = {}\\n',\n",
       "   '        for i in range(len(layers) -1):\\n',\n",
       "   '            print(i, [layers[i], layers[i+1]])\\n',\n",
       "   '            self.layers[f\"lin_{i}\"] = nn.Linear(layers[i], layers[i+1])\\n',\n",
       "   '    def forward(self, xb):\\n',\n",
       "   '        ab = xb\\n',\n",
       "   '        for i in range(len(layers) -1):\\n',\n",
       "   '            ab = F.relu(self.layers[f\"lin_{i}\"](ab))\\n',\n",
       "   '        return ab\\n'],\n",
       "  ['class DeepNN(nn.Module):\\n', '    ### cloze {\\n', '    ### } cloze '])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extractCloze(c_code[14]['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## all together now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_cells = lambda cells, e: [c for c in cells if c['cell_type'] == e]\n",
    "\n",
    "def readNotebook(path):\n",
    "    # read file\n",
    "    f = open(path, 'r')\n",
    "    nb = f.read()\n",
    "    nbd = json.loads(nb)\n",
    "    \n",
    "    \n",
    "    cloze_blocks = []\n",
    "    \n",
    "    for i,c in enumerate(nbd['cells']):\n",
    "        # go inside 1 cell\n",
    "        # extract all cloze blocks\n",
    "        if c['cell_type'] == 'code' and isCloze(c['source']):\n",
    "            processed = extractCloze(c['source'])\n",
    "            \n",
    "            # add multiple clozes inside one cell\n",
    "            for data in processed:\n",
    "                clz_src, cln_src = data\n",
    "                cloze_blocks.append({\"i\":i, \"clz\":clz_src, \"cln_src\":cln_src})\n",
    "            \n",
    "    output = []\n",
    "    \n",
    "    print('cloze_blocks: ', len(cloze_blocks))\n",
    "    \n",
    "    for clz in cloze_blocks:\n",
    "        new_code_cells = deepcopy([c for c in nbd['cells']])\n",
    "        new_code_cells[clz['i']]['source']=clz[\"cln_src\"]\n",
    "        new_nb = deepcopy({k:nbd[k] for k in nbd})\n",
    "        new_nb['cells'] = new_code_cells\n",
    "        \n",
    "        output.append((clz['clz'], new_nb))\n",
    "    \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloze_blocks:  1\n"
     ]
    }
   ],
   "source": [
    "x = readNotebook('./data/cloze.ipynb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloze_blocks:  1\n",
      "['    def __init__(self, layers):\\n', '        super().__init__()\\n', '        print(layers)\\n', '        self.layers = {}\\n', '        for i in range(len(layers) -1):\\n', '            print(i, [layers[i], layers[i+1]])\\n', '            self.layers[f\"lin_{i}\"] = nn.Linear(layers[i], layers[i+1])\\n', '    def forward(self, xb):\\n', '        ab = xb\\n', '        for i in range(len(layers) -1):\\n', '            ab = F.relu(self.layers[f\"lin_{i}\"](ab))\\n', '        return ab\\n']\n"
     ]
    }
   ],
   "source": [
    "# full e2e test!\n",
    "# only one cloze in 1 cell of this notebook\n",
    "\n",
    "def get_json(fpath):\n",
    "    f = open(fpath, 'r')\n",
    "    file = json.loads(f.read())\n",
    "    f.close()\n",
    "    return file\n",
    "\n",
    "# some test data\n",
    "expected_answer = get_json('./data/cloze_answer.json')['answer']\n",
    "expected_extract_nb = get_json('./data/cloze_e.ipynb')\n",
    "\n",
    "answer, extract_nb = readNotebook('./data/cloze.ipynb')[0]\n",
    "\n",
    "\n",
    "print(answer)\n",
    "assert(answer == expected_answer)\n",
    "assert(extract_nb == expected_extract_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multi cloze in single notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cloze_blocks:  3\n",
      "0\n",
      "1\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "idxs = [i for i in range(3)]\n",
    "\n",
    "expected_answers = get_json('./data/multi_answers.json')['answers']\n",
    "expected_extract_nb = [ get_json(f\"./data/multi_cloze-{i+1}.ipynb\") for i in idxs]\n",
    "\n",
    "\n",
    "\n",
    "output = readNotebook('./data/multi_cloze.ipynb')\n",
    "\n",
    "\n",
    "for i in idxs:\n",
    "    answer, extract_nb = output[i]\n",
    "    print(i)\n",
    "    \n",
    "    # extracted answers are correct\n",
    "    assert(answer == expected_answers[i])\n",
    "    \n",
    "    # extracted new nbs are correct\n",
    "    for j,line in enumerate(expected_extract_nb[i]['cells'][17]['source']):\n",
    "\n",
    "        if not extract_nb['cells'][17]['source'][j] == line:\n",
    "            print(extract_nb['cells'][17]['source'][j],  l)\n",
    "            raise Exception(\"line not equal\")\n",
    "        \n",
    "    assert(extract_nb == expected_extract_nb[i])\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['             # training\\n',\n",
       "   '            model.train()\\n',\n",
       "   '            trn_losses,_,_,_ = zip(\\n',\n",
       "   \"                *[self.descend_(xb, yb) for xb, yb in self.data['trn'] ]\\n\",\n",
       "   '            )\\n'],\n",
       "  {'cells': [{'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['# Fit function\\n',\n",
       "      '\\n',\n",
       "      'based off jeremy howards great tutorial! https://pytorch.org/tutorials/beginner/nn_tutorial.html']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Setup']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 24,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['%reload_ext autoreload\\n',\n",
       "      '%autoreload 2\\n',\n",
       "      '%matplotlib inline']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 25,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from matplotlib import pyplot as plt\\n',\n",
       "      'import numpy as np\\n',\n",
       "      '\\n',\n",
       "      'from pathlib import Path\\n',\n",
       "      'import requests\\n',\n",
       "      'import pickle\\n',\n",
       "      'import gzip\\n',\n",
       "      'import torch']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 26,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['DATA_PATH = Path(\"data\")\\n',\n",
       "      'PATH = DATA_PATH / \"mnist\"\\n',\n",
       "      'FILENAME = \"mnist.pkl.gz\"']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 27,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['def unzip_data():\\n',\n",
       "      '    with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\\n',\n",
       "      '            ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\\n',\n",
       "      '    \\n',\n",
       "      '    return x_train, y_train, x_valid, y_valid']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## PyTorch Datasets\\n',\n",
       "      '\\n',\n",
       "      'abstract class where you need to implement\\n',\n",
       "      '\\n',\n",
       "      '__len__ & __getitem__ methods ']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 28,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch.utils.data import Dataset, DataLoader, TensorDataset']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 29,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class MnistDataset(Dataset):\\n',\n",
       "      '    def __init__(self, is_valid=False):\\n',\n",
       "      '        x_train, y_train, x_valid, y_valid = unzip_data()\\n',\n",
       "      '        \\n',\n",
       "      '        self.x = x_train if is_valid else x_valid\\n',\n",
       "      '        self.y = y_train if is_valid else y_valid\\n',\n",
       "      '        self.len = self.x.shape[0] \\n',\n",
       "      '    \\n',\n",
       "      '    def __getitem__(self, index):\\n',\n",
       "      '        return self.x[index], self.y[index]\\n',\n",
       "      '        \\n',\n",
       "      '    def __len__(self):\\n',\n",
       "      '        return self.len']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 30,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['train_ds = MnistDataset()\\n',\n",
       "      'valid_ds = MnistDataset(is_valid=True)']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## PyTorch Dataloader\\n',\n",
       "      '\\n',\n",
       "      'manages getting batches for our training loop from our dataloader\\n',\n",
       "      '\\n',\n",
       "      'returns a python interator']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 31,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class WrappedDataLoader:\\n',\n",
       "      '    def __init__(self, dl, func):\\n',\n",
       "      '        self.dl = dl\\n',\n",
       "      '        self.func = func\\n',\n",
       "      '\\n',\n",
       "      '    def __len__(self):\\n',\n",
       "      '        return len(self.dl)\\n',\n",
       "      '\\n',\n",
       "      '    def __iter__(self):\\n',\n",
       "      '        batches = iter(self.dl)\\n',\n",
       "      '        for b in batches:\\n',\n",
       "      '            yield (self.func(*b))']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 32,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['dev = torch.device(\\n',\n",
       "      '    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n',\n",
       "      '\\n',\n",
       "      'def to_device(x, y):\\n',\n",
       "      '    return x.to(dev), y.to(dev)\\n',\n",
       "      '\\n',\n",
       "      'def get_data(bs):\\n',\n",
       "      '    train_ds = MnistDataset()\\n',\n",
       "      '    valid_ds = MnistDataset(is_valid=True)\\n',\n",
       "      '\\n',\n",
       "      '    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\\n',\n",
       "      '    valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\\n',\n",
       "      '    \\n',\n",
       "      '    train_dl = WrappedDataLoader(train_dl, to_device)\\n',\n",
       "      '    valid_dl = WrappedDataLoader(valid_dl, to_device)\\n',\n",
       "      '    \\n',\n",
       "      '    return train_dl, valid_dl']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 33,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch import nn\\n',\n",
       "      'import torch.nn.functional as F\\n',\n",
       "      '\\n',\n",
       "      'loss_func = F.cross_entropy\\n',\n",
       "      '\\n',\n",
       "      'def accuracy(out, yb):\\n',\n",
       "      '    preds = torch.argmax(out, dim=1)\\n',\n",
       "      '    return (preds == yb).float().mean()\\n',\n",
       "      '\\n',\n",
       "      'class Mnist_NN(nn.Module):\\n',\n",
       "      '    def __init__(self, n_h):\\n',\n",
       "      '        super().__init__()\\n',\n",
       "      '        self.inp = nn.Linear(784, n_h)\\n',\n",
       "      '        self.hid = nn.Linear(n_h, n_h)\\n',\n",
       "      '        self.out = nn.Linear(n_h, 10)\\n',\n",
       "      '        \\n',\n",
       "      '    def forward(self, xb):\\n',\n",
       "      '        ab1 = F.relu(self.inp(xb))\\n',\n",
       "      '        ab2 = F.relu(self.hid(ab1))\\n',\n",
       "      '        ab3 = F.relu(self.out(ab2))\\n',\n",
       "      '        return ab3']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 34,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch import optim\\n',\n",
       "      '\\n',\n",
       "      'def get_model():\\n',\n",
       "      '    model = Mnist_NN(100)\\n',\n",
       "      '    opt = optim.SGD(model.parameters(), lr=0.02)\\n',\n",
       "      '    return model, opt']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## Write loss_batch and fit functions\\n',\n",
       "      '\\n',\n",
       "      '<a id=\"q_1\" >']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['# Learn object']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 40,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['to4dec = lambda a: np.around(a, decimals=4) if not a==None else None\\n',\n",
       "      '\\n',\n",
       "      'listNumpy = lambda l: [x.item() for x in l]\\n',\n",
       "      '\\n',\n",
       "      'sumLoss = lambda loss_list: np.sum(listNumpy(loss_list)) / len(loss_list)\\n',\n",
       "      '\\n',\n",
       "      'caculate_metric = lambda metric: lambda predb, yb, : [ metric(predb, yb).item() for predb, yb in zip(predb, yb)]\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      'def print_epoch_progress(epoch, train_loss=None, valid_loss=None, metrics=[]):\\n',\n",
       "      '    metrics = [f\"{key}: {to4dec(value)}\" for key, value in metrics.items()]\\n',\n",
       "      \"    print(epoch, 'train loss: ', to4dec(train_loss),'valid loss: ', to4dec(valid_loss,), ' '.join(metrics))\\n\",\n",
       "      '\\n',\n",
       "      'class Learner(object):\\n',\n",
       "      '    \"\"\"\\n',\n",
       "      '        Learner object holds model, optimizer and dataloaders\\n',\n",
       "      '    \"\"\"\\n',\n",
       "      '    def __init__(self, model, opt, loss_fn, dls={}, metrics={}):\\n',\n",
       "      '        self.model = model\\n',\n",
       "      '        self.opt = opt\\n',\n",
       "      '        self.loss_fn = loss_fn\\n',\n",
       "      '        # assumes { trn: train_dl, val: valid_dl)\\n',\n",
       "      '        self.data = dls\\n',\n",
       "      '        self.metrics = metrics\\n',\n",
       "      '#         self.schedule\\n',\n",
       "      '        \\n',\n",
       "      '        \\n',\n",
       "      '    def descend_(self, xb, yb, is_valid=False):\\n',\n",
       "      '        \\n',\n",
       "      '        predb = self.model(xb)\\n',\n",
       "      '        loss = self.loss_fn(predb, yb)\\n',\n",
       "      '        \\n',\n",
       "      '        if is_valid is not True:\\n',\n",
       "      '            loss.backward()\\n',\n",
       "      '            self.opt.step()\\n',\n",
       "      '            self.opt.zero_grad()\\n',\n",
       "      '            \\n',\n",
       "      '        return loss, predb, yb,xb\\n',\n",
       "      '        \\n',\n",
       "      '    \\n',\n",
       "      '    def fit_(self, epochs=1):\\n',\n",
       "      '        for epoch in range(epochs):\\n',\n",
       "      '            ### cloze {\\n',\n",
       "      '            ### } cloze\\n',\n",
       "      '            # validation\\n',\n",
       "      '            model.eval()\\n',\n",
       "      '            with torch.no_grad():\\n',\n",
       "      '                val_losses,predb,yb,xb = zip(\\n',\n",
       "      \"                    *[self.descend_(xb, yb, is_valid=True) for xb, yb in self.data['val']]\\n\",\n",
       "      '                )\\n',\n",
       "      '            ### cloze {\\n',\n",
       "      '            training_loss = sumLoss(trn_losses)\\n',\n",
       "      '            validation_loss = sumLoss(val_losses)\\n',\n",
       "      '            ### } cloze\\n',\n",
       "      '            \\n',\n",
       "      '            metrics = { \\n',\n",
       "      '                name:np.mean(caculate_metric(fn)(predb,yb)) for name, fn in self.metrics.items()\\n',\n",
       "      '            }\\n',\n",
       "      '#             = [ m for predb, yb in zip(predb, yb)]\\n',\n",
       "      '    \\n',\n",
       "      \"#             print_epoch_progress(epoch+1, training_loss, validation_loss, {'accuracy': epoch_accuracy}) \\n\",\n",
       "      '            print(epoch+1, training_loss, validation_loss, metrics)']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 41,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['model = Mnist_NN(1000)\\n',\n",
       "      'opt = optim.SGD(model.parameters(), lr=0.02)\\n',\n",
       "      'train_dl, valid_dl = get_data(64)\\n',\n",
       "      \"dls = { 'trn':train_dl, 'val':valid_dl }\\n\",\n",
       "      '\\n',\n",
       "      \"learn = Learner(model, opt, F.cross_entropy, dls, {'accuracy': accuracy})\"]},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 42,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': [\"1 2.230948477034356 2.126170490708802 {'accuracy': 0.4581281969614346}\\n\",\n",
       "        \"2 1.8862776391825098 1.621794290554798 {'accuracy': 0.527525575417082}\\n\",\n",
       "        \"3 1.3676916246960877 1.2349658783744364 {'accuracy': 0.6530690537694165}\\n\",\n",
       "        \"4 1.0872702503659923 1.0524368007164782 {'accuracy': 0.6839593990379588}\\n\",\n",
       "        \"5 0.9456099043985841 0.954960170609262 {'accuracy': 0.6923992966142152}\\n\",\n",
       "        \"6 0.8694153633087304 0.9062317304903894 {'accuracy': 0.7027533568079819}\\n\",\n",
       "        \"7 0.8185388354738806 0.862299663179061 {'accuracy': 0.7086996483375959}\\n\",\n",
       "        \"8 0.7908238852100008 0.8398615146232078 {'accuracy': 0.7110014385579492}\\n\",\n",
       "        \"9 0.7657852683477341 0.8275769012968254 {'accuracy': 0.7138986572280259}\\n\",\n",
       "        \"10 0.7483618466337775 0.8129707870580961 {'accuracy': 0.7166959718060311}\\n\"]}],\n",
       "     'source': ['learn.fit_(10)']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['# L-Layer Model']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 43,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class DeepNN(nn.Module):\\n',\n",
       "      '    ### cloze {\\n',\n",
       "      '    def __init__(self, layers):\\n',\n",
       "      '        super().__init__()\\n',\n",
       "      '        print(layers)\\n',\n",
       "      '        self.layers = {}\\n',\n",
       "      '        for i in range(len(layers) -1):\\n',\n",
       "      '            print(i, [layers[i], layers[i+1]])\\n',\n",
       "      '            self.layers[f\"lin_{i}\"] = nn.Linear(layers[i], layers[i+1])\\n',\n",
       "      '    def forward(self, xb):\\n',\n",
       "      '        ab = xb\\n',\n",
       "      '        for i in range(len(layers) -1):\\n',\n",
       "      '            ab = F.relu(self.layers[f\"lin_{i}\"](ab))\\n',\n",
       "      '        return ab\\n',\n",
       "      '    ### } cloze ']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 44,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': ['[ 784 3136 6272 3136  784  196   98   10]\\n',\n",
       "        '[ 784 3136 6272 3136  784  196   98   10]\\n',\n",
       "        '0 [784, 3136]\\n',\n",
       "        '1 [3136, 6272]\\n',\n",
       "        '2 [6272, 3136]\\n',\n",
       "        '3 [3136, 784]\\n',\n",
       "        '4 [784, 196]\\n',\n",
       "        '5 [196, 98]\\n',\n",
       "        '6 [98, 10]\\n']}],\n",
       "     'source': ['layers = np.array([784, 784*4, 784*8, 784*4, 784, 784/4, 784/8, 10]).astype(int)\\n',\n",
       "      'print(layers)\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      'model = DeepNN(layers)\\n',\n",
       "      \"learn = Learner(model, opt, F.cross_entropy, dls, {'accuracy': accuracy})\"]},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 45,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'data': {'text/plain': [\"{'lin_0': Linear(in_features=784, out_features=3136, bias=True),\\n\",\n",
       "         \" 'lin_1': Linear(in_features=3136, out_features=6272, bias=True),\\n\",\n",
       "         \" 'lin_2': Linear(in_features=6272, out_features=3136, bias=True),\\n\",\n",
       "         \" 'lin_3': Linear(in_features=3136, out_features=784, bias=True),\\n\",\n",
       "         \" 'lin_4': Linear(in_features=784, out_features=196, bias=True),\\n\",\n",
       "         \" 'lin_5': Linear(in_features=196, out_features=98, bias=True),\\n\",\n",
       "         \" 'lin_6': Linear(in_features=98, out_features=10, bias=True)}\"]},\n",
       "       'execution_count': 45,\n",
       "       'metadata': {},\n",
       "       'output_type': 'execute_result'}],\n",
       "     'source': ['model.layers']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 46,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': [\"1 2.30204907648123 2.302555062276933 {'accuracy': 0.10348465472764676}\\n\"]}],\n",
       "     'source': ['learn.fit_()']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': None,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': []}],\n",
       "   'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "     'language': 'python',\n",
       "     'name': 'python3'},\n",
       "    'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "     'file_extension': '.py',\n",
       "     'mimetype': 'text/x-python',\n",
       "     'name': 'python',\n",
       "     'nbconvert_exporter': 'python',\n",
       "     'pygments_lexer': 'ipython3',\n",
       "     'version': '3.6.6'}},\n",
       "   'nbformat': 4,\n",
       "   'nbformat_minor': 2}),\n",
       " (['            training_loss = sumLoss(trn_losses)\\n',\n",
       "   '            validation_loss = sumLoss(val_losses)\\n'],\n",
       "  {'cells': [{'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['# Fit function\\n',\n",
       "      '\\n',\n",
       "      'based off jeremy howards great tutorial! https://pytorch.org/tutorials/beginner/nn_tutorial.html']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Setup']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 24,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['%reload_ext autoreload\\n',\n",
       "      '%autoreload 2\\n',\n",
       "      '%matplotlib inline']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 25,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from matplotlib import pyplot as plt\\n',\n",
       "      'import numpy as np\\n',\n",
       "      '\\n',\n",
       "      'from pathlib import Path\\n',\n",
       "      'import requests\\n',\n",
       "      'import pickle\\n',\n",
       "      'import gzip\\n',\n",
       "      'import torch']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 26,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['DATA_PATH = Path(\"data\")\\n',\n",
       "      'PATH = DATA_PATH / \"mnist\"\\n',\n",
       "      'FILENAME = \"mnist.pkl.gz\"']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 27,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['def unzip_data():\\n',\n",
       "      '    with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\\n',\n",
       "      '            ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\\n',\n",
       "      '    \\n',\n",
       "      '    return x_train, y_train, x_valid, y_valid']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## PyTorch Datasets\\n',\n",
       "      '\\n',\n",
       "      'abstract class where you need to implement\\n',\n",
       "      '\\n',\n",
       "      '__len__ & __getitem__ methods ']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 28,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch.utils.data import Dataset, DataLoader, TensorDataset']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 29,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class MnistDataset(Dataset):\\n',\n",
       "      '    def __init__(self, is_valid=False):\\n',\n",
       "      '        x_train, y_train, x_valid, y_valid = unzip_data()\\n',\n",
       "      '        \\n',\n",
       "      '        self.x = x_train if is_valid else x_valid\\n',\n",
       "      '        self.y = y_train if is_valid else y_valid\\n',\n",
       "      '        self.len = self.x.shape[0] \\n',\n",
       "      '    \\n',\n",
       "      '    def __getitem__(self, index):\\n',\n",
       "      '        return self.x[index], self.y[index]\\n',\n",
       "      '        \\n',\n",
       "      '    def __len__(self):\\n',\n",
       "      '        return self.len']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 30,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['train_ds = MnistDataset()\\n',\n",
       "      'valid_ds = MnistDataset(is_valid=True)']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## PyTorch Dataloader\\n',\n",
       "      '\\n',\n",
       "      'manages getting batches for our training loop from our dataloader\\n',\n",
       "      '\\n',\n",
       "      'returns a python interator']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 31,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class WrappedDataLoader:\\n',\n",
       "      '    def __init__(self, dl, func):\\n',\n",
       "      '        self.dl = dl\\n',\n",
       "      '        self.func = func\\n',\n",
       "      '\\n',\n",
       "      '    def __len__(self):\\n',\n",
       "      '        return len(self.dl)\\n',\n",
       "      '\\n',\n",
       "      '    def __iter__(self):\\n',\n",
       "      '        batches = iter(self.dl)\\n',\n",
       "      '        for b in batches:\\n',\n",
       "      '            yield (self.func(*b))']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 32,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['dev = torch.device(\\n',\n",
       "      '    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n',\n",
       "      '\\n',\n",
       "      'def to_device(x, y):\\n',\n",
       "      '    return x.to(dev), y.to(dev)\\n',\n",
       "      '\\n',\n",
       "      'def get_data(bs):\\n',\n",
       "      '    train_ds = MnistDataset()\\n',\n",
       "      '    valid_ds = MnistDataset(is_valid=True)\\n',\n",
       "      '\\n',\n",
       "      '    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\\n',\n",
       "      '    valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\\n',\n",
       "      '    \\n',\n",
       "      '    train_dl = WrappedDataLoader(train_dl, to_device)\\n',\n",
       "      '    valid_dl = WrappedDataLoader(valid_dl, to_device)\\n',\n",
       "      '    \\n',\n",
       "      '    return train_dl, valid_dl']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 33,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch import nn\\n',\n",
       "      'import torch.nn.functional as F\\n',\n",
       "      '\\n',\n",
       "      'loss_func = F.cross_entropy\\n',\n",
       "      '\\n',\n",
       "      'def accuracy(out, yb):\\n',\n",
       "      '    preds = torch.argmax(out, dim=1)\\n',\n",
       "      '    return (preds == yb).float().mean()\\n',\n",
       "      '\\n',\n",
       "      'class Mnist_NN(nn.Module):\\n',\n",
       "      '    def __init__(self, n_h):\\n',\n",
       "      '        super().__init__()\\n',\n",
       "      '        self.inp = nn.Linear(784, n_h)\\n',\n",
       "      '        self.hid = nn.Linear(n_h, n_h)\\n',\n",
       "      '        self.out = nn.Linear(n_h, 10)\\n',\n",
       "      '        \\n',\n",
       "      '    def forward(self, xb):\\n',\n",
       "      '        ab1 = F.relu(self.inp(xb))\\n',\n",
       "      '        ab2 = F.relu(self.hid(ab1))\\n',\n",
       "      '        ab3 = F.relu(self.out(ab2))\\n',\n",
       "      '        return ab3']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 34,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch import optim\\n',\n",
       "      '\\n',\n",
       "      'def get_model():\\n',\n",
       "      '    model = Mnist_NN(100)\\n',\n",
       "      '    opt = optim.SGD(model.parameters(), lr=0.02)\\n',\n",
       "      '    return model, opt']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## Write loss_batch and fit functions\\n',\n",
       "      '\\n',\n",
       "      '<a id=\"q_1\" >']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['# Learn object']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 40,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['to4dec = lambda a: np.around(a, decimals=4) if not a==None else None\\n',\n",
       "      '\\n',\n",
       "      'listNumpy = lambda l: [x.item() for x in l]\\n',\n",
       "      '\\n',\n",
       "      'sumLoss = lambda loss_list: np.sum(listNumpy(loss_list)) / len(loss_list)\\n',\n",
       "      '\\n',\n",
       "      'caculate_metric = lambda metric: lambda predb, yb, : [ metric(predb, yb).item() for predb, yb in zip(predb, yb)]\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      'def print_epoch_progress(epoch, train_loss=None, valid_loss=None, metrics=[]):\\n',\n",
       "      '    metrics = [f\"{key}: {to4dec(value)}\" for key, value in metrics.items()]\\n',\n",
       "      \"    print(epoch, 'train loss: ', to4dec(train_loss),'valid loss: ', to4dec(valid_loss,), ' '.join(metrics))\\n\",\n",
       "      '\\n',\n",
       "      'class Learner(object):\\n',\n",
       "      '    \"\"\"\\n',\n",
       "      '        Learner object holds model, optimizer and dataloaders\\n',\n",
       "      '    \"\"\"\\n',\n",
       "      '    def __init__(self, model, opt, loss_fn, dls={}, metrics={}):\\n',\n",
       "      '        self.model = model\\n',\n",
       "      '        self.opt = opt\\n',\n",
       "      '        self.loss_fn = loss_fn\\n',\n",
       "      '        # assumes { trn: train_dl, val: valid_dl)\\n',\n",
       "      '        self.data = dls\\n',\n",
       "      '        self.metrics = metrics\\n',\n",
       "      '#         self.schedule\\n',\n",
       "      '        \\n',\n",
       "      '        \\n',\n",
       "      '    def descend_(self, xb, yb, is_valid=False):\\n',\n",
       "      '        \\n',\n",
       "      '        predb = self.model(xb)\\n',\n",
       "      '        loss = self.loss_fn(predb, yb)\\n',\n",
       "      '        \\n',\n",
       "      '        if is_valid is not True:\\n',\n",
       "      '            loss.backward()\\n',\n",
       "      '            self.opt.step()\\n',\n",
       "      '            self.opt.zero_grad()\\n',\n",
       "      '            \\n',\n",
       "      '        return loss, predb, yb,xb\\n',\n",
       "      '        \\n',\n",
       "      '    \\n',\n",
       "      '    def fit_(self, epochs=1):\\n',\n",
       "      '        for epoch in range(epochs):\\n',\n",
       "      '            ### cloze {\\n',\n",
       "      '             # training\\n',\n",
       "      '            model.train()\\n',\n",
       "      '            trn_losses,_,_,_ = zip(\\n',\n",
       "      \"                *[self.descend_(xb, yb) for xb, yb in self.data['trn'] ]\\n\",\n",
       "      '            )\\n',\n",
       "      '            ### } cloze\\n',\n",
       "      '            # validation\\n',\n",
       "      '            model.eval()\\n',\n",
       "      '            with torch.no_grad():\\n',\n",
       "      '                val_losses,predb,yb,xb = zip(\\n',\n",
       "      \"                    *[self.descend_(xb, yb, is_valid=True) for xb, yb in self.data['val']]\\n\",\n",
       "      '                )\\n',\n",
       "      '            ### cloze {\\n',\n",
       "      '            ### } cloze\\n',\n",
       "      '            \\n',\n",
       "      '            metrics = { \\n',\n",
       "      '                name:np.mean(caculate_metric(fn)(predb,yb)) for name, fn in self.metrics.items()\\n',\n",
       "      '            }\\n',\n",
       "      '#             = [ m for predb, yb in zip(predb, yb)]\\n',\n",
       "      '    \\n',\n",
       "      \"#             print_epoch_progress(epoch+1, training_loss, validation_loss, {'accuracy': epoch_accuracy}) \\n\",\n",
       "      '            print(epoch+1, training_loss, validation_loss, metrics)']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 41,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['model = Mnist_NN(1000)\\n',\n",
       "      'opt = optim.SGD(model.parameters(), lr=0.02)\\n',\n",
       "      'train_dl, valid_dl = get_data(64)\\n',\n",
       "      \"dls = { 'trn':train_dl, 'val':valid_dl }\\n\",\n",
       "      '\\n',\n",
       "      \"learn = Learner(model, opt, F.cross_entropy, dls, {'accuracy': accuracy})\"]},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 42,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': [\"1 2.230948477034356 2.126170490708802 {'accuracy': 0.4581281969614346}\\n\",\n",
       "        \"2 1.8862776391825098 1.621794290554798 {'accuracy': 0.527525575417082}\\n\",\n",
       "        \"3 1.3676916246960877 1.2349658783744364 {'accuracy': 0.6530690537694165}\\n\",\n",
       "        \"4 1.0872702503659923 1.0524368007164782 {'accuracy': 0.6839593990379588}\\n\",\n",
       "        \"5 0.9456099043985841 0.954960170609262 {'accuracy': 0.6923992966142152}\\n\",\n",
       "        \"6 0.8694153633087304 0.9062317304903894 {'accuracy': 0.7027533568079819}\\n\",\n",
       "        \"7 0.8185388354738806 0.862299663179061 {'accuracy': 0.7086996483375959}\\n\",\n",
       "        \"8 0.7908238852100008 0.8398615146232078 {'accuracy': 0.7110014385579492}\\n\",\n",
       "        \"9 0.7657852683477341 0.8275769012968254 {'accuracy': 0.7138986572280259}\\n\",\n",
       "        \"10 0.7483618466337775 0.8129707870580961 {'accuracy': 0.7166959718060311}\\n\"]}],\n",
       "     'source': ['learn.fit_(10)']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['# L-Layer Model']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 43,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class DeepNN(nn.Module):\\n',\n",
       "      '    ### cloze {\\n',\n",
       "      '    def __init__(self, layers):\\n',\n",
       "      '        super().__init__()\\n',\n",
       "      '        print(layers)\\n',\n",
       "      '        self.layers = {}\\n',\n",
       "      '        for i in range(len(layers) -1):\\n',\n",
       "      '            print(i, [layers[i], layers[i+1]])\\n',\n",
       "      '            self.layers[f\"lin_{i}\"] = nn.Linear(layers[i], layers[i+1])\\n',\n",
       "      '    def forward(self, xb):\\n',\n",
       "      '        ab = xb\\n',\n",
       "      '        for i in range(len(layers) -1):\\n',\n",
       "      '            ab = F.relu(self.layers[f\"lin_{i}\"](ab))\\n',\n",
       "      '        return ab\\n',\n",
       "      '    ### } cloze ']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 44,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': ['[ 784 3136 6272 3136  784  196   98   10]\\n',\n",
       "        '[ 784 3136 6272 3136  784  196   98   10]\\n',\n",
       "        '0 [784, 3136]\\n',\n",
       "        '1 [3136, 6272]\\n',\n",
       "        '2 [6272, 3136]\\n',\n",
       "        '3 [3136, 784]\\n',\n",
       "        '4 [784, 196]\\n',\n",
       "        '5 [196, 98]\\n',\n",
       "        '6 [98, 10]\\n']}],\n",
       "     'source': ['layers = np.array([784, 784*4, 784*8, 784*4, 784, 784/4, 784/8, 10]).astype(int)\\n',\n",
       "      'print(layers)\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      'model = DeepNN(layers)\\n',\n",
       "      \"learn = Learner(model, opt, F.cross_entropy, dls, {'accuracy': accuracy})\"]},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 45,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'data': {'text/plain': [\"{'lin_0': Linear(in_features=784, out_features=3136, bias=True),\\n\",\n",
       "         \" 'lin_1': Linear(in_features=3136, out_features=6272, bias=True),\\n\",\n",
       "         \" 'lin_2': Linear(in_features=6272, out_features=3136, bias=True),\\n\",\n",
       "         \" 'lin_3': Linear(in_features=3136, out_features=784, bias=True),\\n\",\n",
       "         \" 'lin_4': Linear(in_features=784, out_features=196, bias=True),\\n\",\n",
       "         \" 'lin_5': Linear(in_features=196, out_features=98, bias=True),\\n\",\n",
       "         \" 'lin_6': Linear(in_features=98, out_features=10, bias=True)}\"]},\n",
       "       'execution_count': 45,\n",
       "       'metadata': {},\n",
       "       'output_type': 'execute_result'}],\n",
       "     'source': ['model.layers']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 46,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': [\"1 2.30204907648123 2.302555062276933 {'accuracy': 0.10348465472764676}\\n\"]}],\n",
       "     'source': ['learn.fit_()']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': None,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': []}],\n",
       "   'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "     'language': 'python',\n",
       "     'name': 'python3'},\n",
       "    'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "     'file_extension': '.py',\n",
       "     'mimetype': 'text/x-python',\n",
       "     'name': 'python',\n",
       "     'nbconvert_exporter': 'python',\n",
       "     'pygments_lexer': 'ipython3',\n",
       "     'version': '3.6.6'}},\n",
       "   'nbformat': 4,\n",
       "   'nbformat_minor': 2}),\n",
       " (['    def __init__(self, layers):\\n',\n",
       "   '        super().__init__()\\n',\n",
       "   '        print(layers)\\n',\n",
       "   '        self.layers = {}\\n',\n",
       "   '        for i in range(len(layers) -1):\\n',\n",
       "   '            print(i, [layers[i], layers[i+1]])\\n',\n",
       "   '            self.layers[f\"lin_{i}\"] = nn.Linear(layers[i], layers[i+1])\\n',\n",
       "   '    def forward(self, xb):\\n',\n",
       "   '        ab = xb\\n',\n",
       "   '        for i in range(len(layers) -1):\\n',\n",
       "   '            ab = F.relu(self.layers[f\"lin_{i}\"](ab))\\n',\n",
       "   '        return ab\\n'],\n",
       "  {'cells': [{'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['# Fit function\\n',\n",
       "      '\\n',\n",
       "      'based off jeremy howards great tutorial! https://pytorch.org/tutorials/beginner/nn_tutorial.html']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['## Setup']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 24,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['%reload_ext autoreload\\n',\n",
       "      '%autoreload 2\\n',\n",
       "      '%matplotlib inline']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 25,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from matplotlib import pyplot as plt\\n',\n",
       "      'import numpy as np\\n',\n",
       "      '\\n',\n",
       "      'from pathlib import Path\\n',\n",
       "      'import requests\\n',\n",
       "      'import pickle\\n',\n",
       "      'import gzip\\n',\n",
       "      'import torch']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 26,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['DATA_PATH = Path(\"data\")\\n',\n",
       "      'PATH = DATA_PATH / \"mnist\"\\n',\n",
       "      'FILENAME = \"mnist.pkl.gz\"']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 27,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['def unzip_data():\\n',\n",
       "      '    with gzip.open((PATH / FILENAME).as_posix(), \"rb\") as f:\\n',\n",
       "      '            ((x_train, y_train), (x_valid, y_valid), _) = pickle.load(f, encoding=\"latin-1\")\\n',\n",
       "      '    \\n',\n",
       "      '    return x_train, y_train, x_valid, y_valid']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## PyTorch Datasets\\n',\n",
       "      '\\n',\n",
       "      'abstract class where you need to implement\\n',\n",
       "      '\\n',\n",
       "      '__len__ & __getitem__ methods ']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 28,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch.utils.data import Dataset, DataLoader, TensorDataset']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 29,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class MnistDataset(Dataset):\\n',\n",
       "      '    def __init__(self, is_valid=False):\\n',\n",
       "      '        x_train, y_train, x_valid, y_valid = unzip_data()\\n',\n",
       "      '        \\n',\n",
       "      '        self.x = x_train if is_valid else x_valid\\n',\n",
       "      '        self.y = y_train if is_valid else y_valid\\n',\n",
       "      '        self.len = self.x.shape[0] \\n',\n",
       "      '    \\n',\n",
       "      '    def __getitem__(self, index):\\n',\n",
       "      '        return self.x[index], self.y[index]\\n',\n",
       "      '        \\n',\n",
       "      '    def __len__(self):\\n',\n",
       "      '        return self.len']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 30,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['train_ds = MnistDataset()\\n',\n",
       "      'valid_ds = MnistDataset(is_valid=True)']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## PyTorch Dataloader\\n',\n",
       "      '\\n',\n",
       "      'manages getting batches for our training loop from our dataloader\\n',\n",
       "      '\\n',\n",
       "      'returns a python interator']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 31,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class WrappedDataLoader:\\n',\n",
       "      '    def __init__(self, dl, func):\\n',\n",
       "      '        self.dl = dl\\n',\n",
       "      '        self.func = func\\n',\n",
       "      '\\n',\n",
       "      '    def __len__(self):\\n',\n",
       "      '        return len(self.dl)\\n',\n",
       "      '\\n',\n",
       "      '    def __iter__(self):\\n',\n",
       "      '        batches = iter(self.dl)\\n',\n",
       "      '        for b in batches:\\n',\n",
       "      '            yield (self.func(*b))']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 32,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['dev = torch.device(\\n',\n",
       "      '    \"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\\n',\n",
       "      '\\n',\n",
       "      'def to_device(x, y):\\n',\n",
       "      '    return x.to(dev), y.to(dev)\\n',\n",
       "      '\\n',\n",
       "      'def get_data(bs):\\n',\n",
       "      '    train_ds = MnistDataset()\\n',\n",
       "      '    valid_ds = MnistDataset(is_valid=True)\\n',\n",
       "      '\\n',\n",
       "      '    train_dl = DataLoader(train_ds, batch_size=bs, shuffle=True)\\n',\n",
       "      '    valid_dl = DataLoader(valid_ds, batch_size=bs * 2)\\n',\n",
       "      '    \\n',\n",
       "      '    train_dl = WrappedDataLoader(train_dl, to_device)\\n',\n",
       "      '    valid_dl = WrappedDataLoader(valid_dl, to_device)\\n',\n",
       "      '    \\n',\n",
       "      '    return train_dl, valid_dl']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 33,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch import nn\\n',\n",
       "      'import torch.nn.functional as F\\n',\n",
       "      '\\n',\n",
       "      'loss_func = F.cross_entropy\\n',\n",
       "      '\\n',\n",
       "      'def accuracy(out, yb):\\n',\n",
       "      '    preds = torch.argmax(out, dim=1)\\n',\n",
       "      '    return (preds == yb).float().mean()\\n',\n",
       "      '\\n',\n",
       "      'class Mnist_NN(nn.Module):\\n',\n",
       "      '    def __init__(self, n_h):\\n',\n",
       "      '        super().__init__()\\n',\n",
       "      '        self.inp = nn.Linear(784, n_h)\\n',\n",
       "      '        self.hid = nn.Linear(n_h, n_h)\\n',\n",
       "      '        self.out = nn.Linear(n_h, 10)\\n',\n",
       "      '        \\n',\n",
       "      '    def forward(self, xb):\\n',\n",
       "      '        ab1 = F.relu(self.inp(xb))\\n',\n",
       "      '        ab2 = F.relu(self.hid(ab1))\\n',\n",
       "      '        ab3 = F.relu(self.out(ab2))\\n',\n",
       "      '        return ab3']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 34,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['from torch import optim\\n',\n",
       "      '\\n',\n",
       "      'def get_model():\\n',\n",
       "      '    model = Mnist_NN(100)\\n',\n",
       "      '    opt = optim.SGD(model.parameters(), lr=0.02)\\n',\n",
       "      '    return model, opt']},\n",
       "    {'cell_type': 'markdown',\n",
       "     'metadata': {},\n",
       "     'source': ['## Write loss_batch and fit functions\\n',\n",
       "      '\\n',\n",
       "      '<a id=\"q_1\" >']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['# Learn object']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 40,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['to4dec = lambda a: np.around(a, decimals=4) if not a==None else None\\n',\n",
       "      '\\n',\n",
       "      'listNumpy = lambda l: [x.item() for x in l]\\n',\n",
       "      '\\n',\n",
       "      'sumLoss = lambda loss_list: np.sum(listNumpy(loss_list)) / len(loss_list)\\n',\n",
       "      '\\n',\n",
       "      'caculate_metric = lambda metric: lambda predb, yb, : [ metric(predb, yb).item() for predb, yb in zip(predb, yb)]\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      'def print_epoch_progress(epoch, train_loss=None, valid_loss=None, metrics=[]):\\n',\n",
       "      '    metrics = [f\"{key}: {to4dec(value)}\" for key, value in metrics.items()]\\n',\n",
       "      \"    print(epoch, 'train loss: ', to4dec(train_loss),'valid loss: ', to4dec(valid_loss,), ' '.join(metrics))\\n\",\n",
       "      '\\n',\n",
       "      'class Learner(object):\\n',\n",
       "      '    \"\"\"\\n',\n",
       "      '        Learner object holds model, optimizer and dataloaders\\n',\n",
       "      '    \"\"\"\\n',\n",
       "      '    def __init__(self, model, opt, loss_fn, dls={}, metrics={}):\\n',\n",
       "      '        self.model = model\\n',\n",
       "      '        self.opt = opt\\n',\n",
       "      '        self.loss_fn = loss_fn\\n',\n",
       "      '        # assumes { trn: train_dl, val: valid_dl)\\n',\n",
       "      '        self.data = dls\\n',\n",
       "      '        self.metrics = metrics\\n',\n",
       "      '#         self.schedule\\n',\n",
       "      '        \\n',\n",
       "      '        \\n',\n",
       "      '    def descend_(self, xb, yb, is_valid=False):\\n',\n",
       "      '        \\n',\n",
       "      '        predb = self.model(xb)\\n',\n",
       "      '        loss = self.loss_fn(predb, yb)\\n',\n",
       "      '        \\n',\n",
       "      '        if is_valid is not True:\\n',\n",
       "      '            loss.backward()\\n',\n",
       "      '            self.opt.step()\\n',\n",
       "      '            self.opt.zero_grad()\\n',\n",
       "      '            \\n',\n",
       "      '        return loss, predb, yb,xb\\n',\n",
       "      '        \\n',\n",
       "      '    \\n',\n",
       "      '    def fit_(self, epochs=1):\\n',\n",
       "      '        for epoch in range(epochs):\\n',\n",
       "      '            ### cloze {\\n',\n",
       "      '             # training\\n',\n",
       "      '            model.train()\\n',\n",
       "      '            trn_losses,_,_,_ = zip(\\n',\n",
       "      \"                *[self.descend_(xb, yb) for xb, yb in self.data['trn'] ]\\n\",\n",
       "      '            )\\n',\n",
       "      '            ### } cloze\\n',\n",
       "      '            # validation\\n',\n",
       "      '            model.eval()\\n',\n",
       "      '            with torch.no_grad():\\n',\n",
       "      '                val_losses,predb,yb,xb = zip(\\n',\n",
       "      \"                    *[self.descend_(xb, yb, is_valid=True) for xb, yb in self.data['val']]\\n\",\n",
       "      '                )\\n',\n",
       "      '            ### cloze {\\n',\n",
       "      '            training_loss = sumLoss(trn_losses)\\n',\n",
       "      '            validation_loss = sumLoss(val_losses)\\n',\n",
       "      '            ### } cloze\\n',\n",
       "      '            \\n',\n",
       "      '            metrics = { \\n',\n",
       "      '                name:np.mean(caculate_metric(fn)(predb,yb)) for name, fn in self.metrics.items()\\n',\n",
       "      '            }\\n',\n",
       "      '#             = [ m for predb, yb in zip(predb, yb)]\\n',\n",
       "      '    \\n',\n",
       "      \"#             print_epoch_progress(epoch+1, training_loss, validation_loss, {'accuracy': epoch_accuracy}) \\n\",\n",
       "      '            print(epoch+1, training_loss, validation_loss, metrics)']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 41,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['model = Mnist_NN(1000)\\n',\n",
       "      'opt = optim.SGD(model.parameters(), lr=0.02)\\n',\n",
       "      'train_dl, valid_dl = get_data(64)\\n',\n",
       "      \"dls = { 'trn':train_dl, 'val':valid_dl }\\n\",\n",
       "      '\\n',\n",
       "      \"learn = Learner(model, opt, F.cross_entropy, dls, {'accuracy': accuracy})\"]},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 42,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': [\"1 2.230948477034356 2.126170490708802 {'accuracy': 0.4581281969614346}\\n\",\n",
       "        \"2 1.8862776391825098 1.621794290554798 {'accuracy': 0.527525575417082}\\n\",\n",
       "        \"3 1.3676916246960877 1.2349658783744364 {'accuracy': 0.6530690537694165}\\n\",\n",
       "        \"4 1.0872702503659923 1.0524368007164782 {'accuracy': 0.6839593990379588}\\n\",\n",
       "        \"5 0.9456099043985841 0.954960170609262 {'accuracy': 0.6923992966142152}\\n\",\n",
       "        \"6 0.8694153633087304 0.9062317304903894 {'accuracy': 0.7027533568079819}\\n\",\n",
       "        \"7 0.8185388354738806 0.862299663179061 {'accuracy': 0.7086996483375959}\\n\",\n",
       "        \"8 0.7908238852100008 0.8398615146232078 {'accuracy': 0.7110014385579492}\\n\",\n",
       "        \"9 0.7657852683477341 0.8275769012968254 {'accuracy': 0.7138986572280259}\\n\",\n",
       "        \"10 0.7483618466337775 0.8129707870580961 {'accuracy': 0.7166959718060311}\\n\"]}],\n",
       "     'source': ['learn.fit_(10)']},\n",
       "    {'cell_type': 'markdown', 'metadata': {}, 'source': ['# L-Layer Model']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 43,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': ['class DeepNN(nn.Module):\\n',\n",
       "      '    ### cloze {\\n',\n",
       "      '    ### } cloze ']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 44,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': ['[ 784 3136 6272 3136  784  196   98   10]\\n',\n",
       "        '[ 784 3136 6272 3136  784  196   98   10]\\n',\n",
       "        '0 [784, 3136]\\n',\n",
       "        '1 [3136, 6272]\\n',\n",
       "        '2 [6272, 3136]\\n',\n",
       "        '3 [3136, 784]\\n',\n",
       "        '4 [784, 196]\\n',\n",
       "        '5 [196, 98]\\n',\n",
       "        '6 [98, 10]\\n']}],\n",
       "     'source': ['layers = np.array([784, 784*4, 784*8, 784*4, 784, 784/4, 784/8, 10]).astype(int)\\n',\n",
       "      'print(layers)\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      '\\n',\n",
       "      'model = DeepNN(layers)\\n',\n",
       "      \"learn = Learner(model, opt, F.cross_entropy, dls, {'accuracy': accuracy})\"]},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 45,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'data': {'text/plain': [\"{'lin_0': Linear(in_features=784, out_features=3136, bias=True),\\n\",\n",
       "         \" 'lin_1': Linear(in_features=3136, out_features=6272, bias=True),\\n\",\n",
       "         \" 'lin_2': Linear(in_features=6272, out_features=3136, bias=True),\\n\",\n",
       "         \" 'lin_3': Linear(in_features=3136, out_features=784, bias=True),\\n\",\n",
       "         \" 'lin_4': Linear(in_features=784, out_features=196, bias=True),\\n\",\n",
       "         \" 'lin_5': Linear(in_features=196, out_features=98, bias=True),\\n\",\n",
       "         \" 'lin_6': Linear(in_features=98, out_features=10, bias=True)}\"]},\n",
       "       'execution_count': 45,\n",
       "       'metadata': {},\n",
       "       'output_type': 'execute_result'}],\n",
       "     'source': ['model.layers']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': 46,\n",
       "     'metadata': {},\n",
       "     'outputs': [{'name': 'stdout',\n",
       "       'output_type': 'stream',\n",
       "       'text': [\"1 2.30204907648123 2.302555062276933 {'accuracy': 0.10348465472764676}\\n\"]}],\n",
       "     'source': ['learn.fit_()']},\n",
       "    {'cell_type': 'code',\n",
       "     'execution_count': None,\n",
       "     'metadata': {},\n",
       "     'outputs': [],\n",
       "     'source': []}],\n",
       "   'metadata': {'kernelspec': {'display_name': 'Python 3',\n",
       "     'language': 'python',\n",
       "     'name': 'python3'},\n",
       "    'language_info': {'codemirror_mode': {'name': 'ipython', 'version': 3},\n",
       "     'file_extension': '.py',\n",
       "     'mimetype': 'text/x-python',\n",
       "     'name': 'python',\n",
       "     'nbconvert_exporter': 'python',\n",
       "     'pygments_lexer': 'ipython3',\n",
       "     'version': '3.6.6'}},\n",
       "   'nbformat': 4,\n",
       "   'nbformat_minor': 2})]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
